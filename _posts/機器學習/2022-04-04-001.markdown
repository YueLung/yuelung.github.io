---
layout: post
title: Linear Regression
date: 2022-04-04 23:30:23 +0800
category: 機器學習
---
### linear Regression
Linear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It’s used to predict values within a continuous range, (e.g. sales, price) 
![alt text](/public/img/機器學習/001-linearRegression-001.PNG)
<br>
### Cost Function 
Let’s use MSE,(Mean-Square Error,均方誤差)
![alt text](/public/img/機器學習/001-linearRegression-002.PNG)
> N is the total number of observations (data points)   

> yi is the actual value of an observation and mxi+b is our prediction
    
<br>
### Gradient descent
To minimize MSE we use Gradient Descent to calculate the gradient of our cost function. 
![alt text](/public/img/機器學習/001-linearRegression-003.PNG)
<br>
using the derivative of the cost function to find the gradient 
(The slope of the cost function using our current weight), 
and then changing our weight to move in the direction opposite of the gradient.
![alt text](/public/img/機器學習/001-linearRegression-004.PNG)

### 實作
```c++
#include <iostream>
#include <vector>
#include <math.h>

int main()
{
    double learnRate = 0.0001;
    std::vector<std::vector<double>> x = 
    {
        {37.8, 1},
        {39.3, 1},
        {45.9, 1},
        {41.3, 1},
    };
    std::vector<double> y = {22.1,10.4,18.3,18.5}; 
    std::vector<double> w = {0.01,5}; 

    for (int i = 0; i < 3000; ++i)
    {
        std::vector<double> derivativeCost;
        for (double it : w) derivativeCost.push_back(0);
        for (int dataCount = 0; dataCount < x.size(); ++dataCount)
        {
            for (int k = 0; k < derivativeCost.size(); ++k)
            {
                double sum = 0;
                for (int a = 0; a < x[0].size(); ++a)
                    sum += x[dataCount][a] * w[a];       
                derivativeCost[k] += (y[dataCount] - sum) * -1 * x[dataCount][k];
            }
        }
        for (double it : derivativeCost)
            it /= x.size();
        
        for (int j = 0 ; j < w.size(); ++j)
            w[j] = w[j] - learnRate * derivativeCost[j];
        
        if ( i % 1000 == 0)
        {
            int totalCost = 0;
            for (int j = 0; j < x.size(); ++j)
            {
                int sum = 0;
                for (int a = 0; a < x[0].size() ;++a)
                    sum += x[j][a] * w[a];  
                totalCost += pow((y[j] - sum), 2) / x[0].size();
            }
            std::cout << "episode = "<< i <<" cost = " << totalCost;
            for (int i = 0; i < w.size(); ++i)
                std::cout << " w" << i << " = "<< w[i];
            std::cout << std::endl;    
        }
    }
    
    for (int i = 0; i < w.size(); ++i)
        std::cout << "w" << i << " = "<< w[i] <<std::endl;
        
    for (int i = 0; i < x.size(); ++i)
    {
        double val = 0;
        for (int j = 0; j < x[i].size(); ++j)
        {
            std::cout << "x" << j << " = "<< x[i][j] <<" ";
            val += x[i][j] * w[j];
        }
        std::cout << "actVal = " << y[i] <<" cal = " << val << std::endl;      
    }    
}

output:
episode = 0 cost = 78 w0 = 0.205876 w1 = 5.00477
episode = 1000 cost = 34 w0 = 0.297928 w1 = 5.03006
episode = 2000 cost = 34 w0 = 0.297371 w1 = 5.05306
w0 = 0.296816
w1 = 5.07598
x0 = 37.8 x1 = 1 actVal = 22.1 cal = 16.2956
x0 = 39.3 x1 = 1 actVal = 10.4 cal = 16.7408
x0 = 45.9 x1 = 1 actVal = 18.3 cal = 18.6998
x0 = 41.3 x1 = 1 actVal = 18.5 cal = 17.3345
```
> y = 0.296816*x + 5.07598
